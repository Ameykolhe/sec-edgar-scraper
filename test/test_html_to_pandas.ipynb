{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a3c915cfbe44b3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sec_edgar_scraper import SecEdgarScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "name = \"Kshitij\"\n",
    "email = \"kshitij0903@gmail.com\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": f\"{name} {email}\"\n",
    "}\n",
    "\n",
    "#https://www.sec.gov/Archives/edgar/data/1045810/000104581024000029/R9.htm\n",
    "# init scraper object\n",
    "sec_scraper = SecEdgarScraper(name, email)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T04:00:29.799975Z",
     "start_time": "2024-04-05T04:00:29.779476Z"
    }
   },
   "id": "427e362a6755d5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ticker = \"nvda\"\n",
    "cik = sec_scraper.get_cik_matching_ticker(ticker)\n",
    "accession_number = \"000104581015000036\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d972025c2c898c75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup = sec_scraper.get_statement_soup(cik, accession_number, \"balance_sheet\")\n",
    "\n",
    "table = soup.find(\"table\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74204560d00e111b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df1 = pd.read_html(StringIO(str(table)))[0]\n",
    "\n",
    "df1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17c73a76a800fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resp2 = requests.get(\"https://www.sec.gov/Archives/edgar/data/1045810/000104581024000029/R9.htm\", headers=headers)\n",
    "soup2 = BeautifulSoup(resp2.content, \"lxml\")\n",
    "\n",
    "table2 = soup2.find(\"table\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9798d9d1e07986ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2 = pd.read_html(StringIO(str(table2)))[0]\n",
    "\n",
    "df2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e2a8001a4252526"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resp3 = requests.get(\"https://www.sec.gov/Archives/edgar/data/1045810/000104581024000029/R9.htm\", headers=headers)\n",
    "soup3 = BeautifulSoup(resp3.content, \"lxml\")\n",
    "\n",
    "table3 = soup3.find(\"table\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbdd93e1af092fde"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df3 = pd.read_html(StringIO(str(table3)))[0]\n",
    "\n",
    "df3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "102ea4fdbab0f533"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Checking dict structure for different accession_numbers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af95283869a3885a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ticker = nvda"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f28b16c49f23565"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "GetRequestException",
     "evalue": "Retry count exhausted",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTooManyRequestException\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:16\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m http\u001B[38;5;241m.\u001B[39mHTTPStatus\u001B[38;5;241m.\u001B[39mTOO_MANY_REQUESTS:\n\u001B[0;32m---> 16\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m TooManyRequestException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHttp Status code = 429\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp_json:\n",
      "\u001B[0;31mTooManyRequestException\u001B[0m: Http Status code = 429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTooManyRequestException\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:16\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m http\u001B[38;5;241m.\u001B[39mHTTPStatus\u001B[38;5;241m.\u001B[39mTOO_MANY_REQUESTS:\n\u001B[0;32m---> 16\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m TooManyRequestException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHttp Status code = 429\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp_json:\n",
      "\u001B[0;31mTooManyRequestException\u001B[0m: Http Status code = 429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTooManyRequestException\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:16\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m http\u001B[38;5;241m.\u001B[39mHTTPStatus\u001B[38;5;241m.\u001B[39mTOO_MANY_REQUESTS:\n\u001B[0;32m---> 16\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m TooManyRequestException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHttp Status code = 429\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp_json:\n",
      "\u001B[0;31mTooManyRequestException\u001B[0m: Http Status code = 429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mGetRequestException\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m cik \u001B[38;5;241m=\u001B[39m sec_scraper\u001B[38;5;241m.\u001B[39mget_cik_matching_ticker(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnvda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m accession_numbers \u001B[38;5;241m=\u001B[39m \u001B[43msec_scraper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_filtered_filings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcik\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m10-Q\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m accession_number \u001B[38;5;129;01min\u001B[39;00m accession_numbers:\n\u001B[1;32m      7\u001B[0m     df \u001B[38;5;241m=\u001B[39m sec_scraper\u001B[38;5;241m.\u001B[39mget_one_statement(cik, accession_number, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalance_sheet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/scraper.py:106\u001B[0m, in \u001B[0;36mget_filtered_filings\u001B[0;34m(self, cik, filing, just_accession_numbers)\u001B[0m\n\u001B[1;32m    102\u001B[0m     else:\n\u001B[1;32m    103\u001B[0m         return company_json\n\u001B[1;32m    105\u001B[0m def get_filtered_filings(self, cik, filing=None, just_accession_numbers=True):\n\u001B[0;32m--> 106\u001B[0m \n\u001B[1;32m    107\u001B[0m     # Fetch submission data for the given ticker\n\u001B[1;32m    108\u001B[0m     company_filings_df = self.__get_submission_data_for_ticker(cik, only_filings_df=True)\n\u001B[1;32m    110\u001B[0m     # Filter for 10-K or 10-Q forms\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/scraper.py:97\u001B[0m, in \u001B[0;36m__get_submission_data_for_ticker\u001B[0;34m(self, cik, only_filings_df)\u001B[0m\n\u001B[1;32m      0\u001B[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:26\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m TooManyRequestException:\n\u001B[1;32m     25\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 26\u001B[0m     \u001B[43mmake_get_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry_count\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GetRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:26\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m TooManyRequestException:\n\u001B[1;32m     25\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 26\u001B[0m     \u001B[43mmake_get_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry_count\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GetRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:26\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m TooManyRequestException:\n\u001B[1;32m     25\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 26\u001B[0m     \u001B[43mmake_get_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry_count\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GetRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:29\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     26\u001B[0m     make_get_request(url, headers, retry_count \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GetRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m---> 29\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m GetRequestException(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:22\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     20\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mcontent\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 22\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m GetRequestException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetry count exhausted\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m TooManyRequestException:\n\u001B[1;32m     25\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mGetRequestException\u001B[0m: Retry count exhausted"
     ]
    }
   ],
   "source": [
    "cik = sec_scraper.get_cik_matching_ticker(\"nvda\")\n",
    "\n",
    "accession_numbers = sec_scraper.get_filtered_filings(cik, \"10-Q\")\n",
    "\n",
    "\n",
    "for accession_number in accession_numbers:\n",
    "    df = sec_scraper.get_one_statement(cik, accession_number, \"balance_sheet\")\n",
    "    df.to_csv(f\"{accession_number}-balance_sheet.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T04:01:45.416673Z",
     "start_time": "2024-04-05T04:01:41.807782Z"
    }
   },
   "id": "399d9893c1e7b923"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "GetRequestException",
     "evalue": "Retry count exhausted",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTooManyRequestException\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:16\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m http\u001B[38;5;241m.\u001B[39mHTTPStatus\u001B[38;5;241m.\u001B[39mTOO_MANY_REQUESTS:\n\u001B[0;32m---> 16\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m TooManyRequestException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHttp Status code = 429\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp_json:\n",
      "\u001B[0;31mTooManyRequestException\u001B[0m: Http Status code = 429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTooManyRequestException\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:16\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m http\u001B[38;5;241m.\u001B[39mHTTPStatus\u001B[38;5;241m.\u001B[39mTOO_MANY_REQUESTS:\n\u001B[0;32m---> 16\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m TooManyRequestException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHttp Status code = 429\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp_json:\n",
      "\u001B[0;31mTooManyRequestException\u001B[0m: Http Status code = 429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTooManyRequestException\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:16\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m http\u001B[38;5;241m.\u001B[39mHTTPStatus\u001B[38;5;241m.\u001B[39mTOO_MANY_REQUESTS:\n\u001B[0;32m---> 16\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m TooManyRequestException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHttp Status code = 429\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resp_json:\n",
      "\u001B[0;31mTooManyRequestException\u001B[0m: Http Status code = 429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mGetRequestException\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m cik \u001B[38;5;241m=\u001B[39m sec_scraper\u001B[38;5;241m.\u001B[39mget_cik_matching_ticker(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnvda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m accession_numbers \u001B[38;5;241m=\u001B[39m \u001B[43msec_scraper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_filtered_filings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcik\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m10-Q\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m accession_number \u001B[38;5;129;01min\u001B[39;00m accession_numbers:\n\u001B[1;32m      7\u001B[0m     df \u001B[38;5;241m=\u001B[39m sec_scraper\u001B[38;5;241m.\u001B[39mget_one_statement(cik, accession_number, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcash_flow_statement\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/scraper.py:106\u001B[0m, in \u001B[0;36mget_filtered_filings\u001B[0;34m(self, cik, filing, just_accession_numbers)\u001B[0m\n\u001B[1;32m    102\u001B[0m     else:\n\u001B[1;32m    103\u001B[0m         return company_json\n\u001B[1;32m    105\u001B[0m def get_filtered_filings(self, cik, filing=None, just_accession_numbers=True):\n\u001B[0;32m--> 106\u001B[0m \n\u001B[1;32m    107\u001B[0m     # Fetch submission data for the given ticker\n\u001B[1;32m    108\u001B[0m     company_filings_df = self.__get_submission_data_for_ticker(cik, only_filings_df=True)\n\u001B[1;32m    110\u001B[0m     # Filter for 10-K or 10-Q forms\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/scraper.py:97\u001B[0m, in \u001B[0;36m__get_submission_data_for_ticker\u001B[0;34m(self, cik, only_filings_df)\u001B[0m\n\u001B[1;32m      0\u001B[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:26\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m TooManyRequestException:\n\u001B[1;32m     25\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 26\u001B[0m     \u001B[43mmake_get_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry_count\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GetRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:26\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m TooManyRequestException:\n\u001B[1;32m     25\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 26\u001B[0m     \u001B[43mmake_get_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry_count\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GetRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:26\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m TooManyRequestException:\n\u001B[1;32m     25\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 26\u001B[0m     \u001B[43mmake_get_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry_count\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GetRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:29\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     26\u001B[0m     make_get_request(url, headers, retry_count \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GetRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m---> 29\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m GetRequestException(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[0;32m~/NYU/OnCampus/sec-edgar-scraper/src/sec_edgar_scraper/utils.py:22\u001B[0m, in \u001B[0;36mmake_get_request\u001B[0;34m(url, headers, retry_count, payload, resp_json)\u001B[0m\n\u001B[1;32m     20\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mcontent\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 22\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m GetRequestException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetry count exhausted\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m TooManyRequestException:\n\u001B[1;32m     25\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mGetRequestException\u001B[0m: Retry count exhausted"
     ]
    }
   ],
   "source": [
    "cik = sec_scraper.get_cik_matching_ticker(\"nvda\")\n",
    "\n",
    "accession_numbers = sec_scraper.get_filtered_filings(cik, \"10-Q\")\n",
    "\n",
    "\n",
    "for accession_number in accession_numbers:\n",
    "    df = sec_scraper.get_one_statement(cik, accession_number, \"cash_flow_statement\")\n",
    "    df.to_csv(f\"{accession_number}-balance_sheet.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T03:59:31.608113Z",
     "start_time": "2024-04-05T03:59:27.683039Z"
    }
   },
   "id": "ce3873523cade4b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cik = sec_scraper.get_cik_matching_ticker(\"nvda\")\n",
    "\n",
    "accession_numbers = sec_scraper.get_filtered_filings(cik, \"10-Q\")\n",
    "\n",
    "\n",
    "for accession_number in accession_numbers:\n",
    "    df = sec_scraper.get_one_statement(cik, accession_number, \"income_statement\")\n",
    "    df.to_csv(f\"{accession_number}-balance_sheet.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71e4d8618f18624f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ticker = tsla"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c52fd3673af4b99b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cik = sec_scraper.get_cik_matching_ticker(\"tsla\")\n",
    "\n",
    "accession_numbers = sec_scraper.get_filtered_filings(cik, \"10-Q\")\n",
    "\n",
    "\n",
    "for accession_number in accession_numbers:\n",
    "    print(accession_number)\n",
    "    soup4 = sec_scraper.get_statement_soup(cik, accession_number, \"balance_sheet\")\n",
    "    table4 = soup4.find(\"table\")\n",
    "    df4 = pd.read_html(StringIO(str(table4)))[0]\n",
    "    \n",
    "    df4_dict = df4.to_dict()\n",
    "    for key,value in df4_dict.items():\n",
    "        print(key, value, sep=\"\\n\", end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fedf4eccafdbf78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cik = sec_scraper.get_cik_matching_ticker(\"tsla\")\n",
    "\n",
    "accession_numbers = sec_scraper.get_filtered_filings(cik, \"10-Q\")\n",
    "\n",
    "\n",
    "for accession_number in accession_numbers:\n",
    "    print(accession_number)\n",
    "    soup4 = sec_scraper.get_statement_soup(cik, accession_number, \"cash_flow_statement\")\n",
    "    table4 = soup4.find(\"table\")\n",
    "    df4 = pd.read_html(StringIO(str(table4)))[0]\n",
    "    \n",
    "    df4_dict = df4.to_dict()\n",
    "    for key,value in df4_dict.items():\n",
    "        print(key, value, sep=\"\\n\", end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7e71e14f518723f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cik = sec_scraper.get_cik_matching_ticker(\"tsla\")\n",
    "\n",
    "accession_numbers = sec_scraper.get_filtered_filings(cik, \"10-Q\")\n",
    "\n",
    "\n",
    "for accession_number in accession_numbers:\n",
    "    print(accession_number)\n",
    "    soup4 = sec_scraper.get_statement_soup(cik, accession_number, \"income_statement\")\n",
    "    table4 = soup4.find(\"table\")\n",
    "    df4 = pd.read_html(StringIO(str(table4)))[0]\n",
    "    \n",
    "    df4_dict = df4.to_dict()\n",
    "    for key,value in df4_dict.items():\n",
    "        print(key, value, sep=\"\\n\", end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "344a900b59ee17ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fe51277d9842a745"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
